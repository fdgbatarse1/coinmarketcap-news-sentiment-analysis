{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"792e92b6\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Bitcoin Sentiment Analysis with FinBERT\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook implements sentiment analysis on Bitcoin-related text using the FinBERT model.\\n\",\n",
    "    \"We fine-tune the pre-trained FinBERT model on a Bitcoin sentiment dataset and evaluate\\n\",\n",
    "    \"its performance using balanced accuracy and accuracy metrics.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5dc9111d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Import libraries for sentiment analysis using FinBERT with PyTorch, HuggingFace transformers, and evaluation metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"51b982b5\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:32.448708Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:28.557052Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"from loguru import logger\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from datasets import load_dataset\\n\",\n",
    "    \"from transformers import (\\n\",\n",
    "    \"    pipeline,\\n\",\n",
    "    \"    AutoTokenizer,\\n\",\n",
    "    \"    Trainer,\\n\",\n",
    "    \"    TrainingArguments,\\n\",\n",
    "    \"    AutoModelForSequenceClassification,\\n\",\n",
    "    \")\\n\",\n",
    "    \"from sklearn.metrics import balanced_accuracy_score, accuracy_score\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\paypa\\\\OneDrive\\\\Desktop\\\\Project\\\\coinmarketcap-news-sentiment-analysis\\\\.venv\\\\Lib\\\\site-packages\\\\tqdm\\\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\n\",\n",
    "      \"  from .autonotebook import tqdm as notebook_tqdm\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 1\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5b51eb7f\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Set the pre-trained FinBERT model for financial sentiment analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"77b9a8ae\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:32.460250Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:32.457250Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"model_name = \\\"yiyanghkust/finbert-tone\\\"\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 2\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e7dd6b2d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Check CUDA availability and set device for GPU acceleration or fallback to CPU\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"4ea2ef60\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:32.864268Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:32.845976Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    logger.info(\\\"CUDA available. GPU will be used for computation.\\\")\\n\",\n",
    "    \"    device = 0\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    logger.info(\\\"CUDA not available. Using CPU for computation.\\\")\\n\",\n",
    "    \"    device = -1\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001B[32m2025-07-17 18:39:32.860\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m2\\u001B[0m - \\u001B[1mCUDA available. GPU will be used for computation.\\u001B[0m\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 3\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"220d1a39\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Initialize sentiment analysis pipeline with FinBERT model and test with sample text\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"01f4600d\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:36.337517Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:32.875690Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"sentiment_pipeline = pipeline(\\n\",\n",
    "    \"    task=\\\"sentiment-analysis\\\", model=model_name, batch_size=128, device=device\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"result = sentiment_pipeline(\\\"I love you\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(result)\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Device set to use cuda:0\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.334\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m7\\u001B[0m - \\u001B[1m[{'label': 'Positive', 'score': 0.9885214567184448}]\\u001B[0m\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 4\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dd184615\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Load Bitcoin sentiment dataset, split into train/val/test sets, preprocess text data, and prepare for model training\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"4127d613\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:40.521122Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:36.347559Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Load local dataset from parquet file\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from datasets import Dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the local parquet file\\n\",\n",
    "    \"data_path = os.path.join('..', 'backend', 'src', 'data', 'clean', 'cryptopanic_news_clean_with_labels.parquet')\\n\",\n",
    "    \"df = pd.read_parquet(data_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# First, let's check what columns we have\\n\",\n",
    "    \"logger.info(f\\\"Available columns: {list(df.columns)}\\\")\\n\",\n",
    "    \"logger.info(f\\\"Dataset shape: {df.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Identify text and label columns\\n\",\n",
    "    \"text_column = \\\"description\\\"\\n\",\n",
    "    \"label_column = \\\"sentiment\\\" \\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"Using text column: '{text_column}' and label column: '{label_column}'\\\")\\n\",\n",
    "    \"logger.info(f\\\"Unique labels: {df[label_column].value_counts().to_dict()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare the dataset with required columns\\n\",\n",
    "    \"df_prepared = df[[text_column, label_column]].copy()\\n\",\n",
    "    \"df_prepared.columns = ['text', 'labels']  # Rename to standard names\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove any rows with missing values\\n\",\n",
    "    \"df_prepared = df_prepared.dropna()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert to Hugging Face Dataset\\n\",\n",
    "    \"full_dataset = Dataset.from_pandas(df_prepared, preserve_index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split into train/val/test sets (60%/20%/20%)\\n\",\n",
    "    \"total_samples = len(full_dataset)\\n\",\n",
    "    \"train_size = int(0.6 * total_samples)\\n\",\n",
    "    \"val_size = int(0.2 * total_samples)\\n\",\n",
    "    \"test_size = total_samples - train_size - val_size\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Shuffle before splitting for better distribution\\n\",\n",
    "    \"full_dataset = full_dataset.shuffle(seed=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ds_train = full_dataset.select(range(train_size))\\n\",\n",
    "    \"ds_val = full_dataset.select(range(train_size, train_size + val_size))\\n\",\n",
    "    \"ds_test = full_dataset.select(\\n\",\n",
    "    \"    range(train_size + val_size, train_size + val_size + test_size)\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"Train size: {len(ds_train)}, Val size: {len(ds_val)}, Test size: {len(ds_test)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize tokenizer\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create label mappings\\n\",\n",
    "    \"all_labels = set()\\n\",\n",
    "    \"for split in [ds_train, ds_val, ds_test]:\\n\",\n",
    "    \"    for example in split:\\n\",\n",
    "    \"        all_labels.add(example[\\\"labels\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"label_to_id = {label: idx for idx, label in enumerate(sorted(all_labels))}\\n\",\n",
    "    \"id_to_label = {idx: label for label, idx in label_to_id.items()}\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"Label mappings: {label_to_id}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"def convert_labels_to_ids(examples):\\n\",\n",
    "    \"    examples[\\\"labels\\\"] = [label_to_id[label] for label in examples[\\\"labels\\\"]]\\n\",\n",
    "    \"    return examples\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert labels to IDs\\n\",\n",
    "    \"ds_train = ds_train.map(convert_labels_to_ids, batched=True)\\n\",\n",
    "    \"ds_val = ds_val.map(convert_labels_to_ids, batched=True)\\n\",\n",
    "    \"ds_test = ds_test.map(convert_labels_to_ids, batched=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenize the text\\n\",\n",
    "    \"def tokenize_function(examples):\\n\",\n",
    "    \"    tokenized = tokenizer(\\n\",\n",
    "    \"        examples[\\\"text\\\"], truncation=True, padding=\\\"max_length\\\", max_length=128\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    return tokenized\\n\",\n",
    "    \"\\n\",\n",
    "    \"ds_train = ds_train.map(tokenize_function, batched=True)\\n\",\n",
    "    \"ds_val = ds_val.map(tokenize_function, batched=True)\\n\",\n",
    "    \"ds_test = ds_test.map(tokenize_function, batched=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set format for PyTorch\\n\",\n",
    "    \"ds_train.set_format(\\n\",\n",
    "    \"    type=\\\"torch\\\",\\n\",\n",
    "    \"    columns=[\\\"input_ids\\\", \\\"token_type_ids\\\", \\\"attention_mask\\\", \\\"labels\\\"],\\n\",\n",
    "    \")\\n\",\n",
    "    \"ds_val.set_format(\\n\",\n",
    "    \"    type=\\\"torch\\\",\\n\",\n",
    "    \"    columns=[\\\"input_ids\\\", \\\"token_type_ids\\\", \\\"attention_mask\\\", \\\"labels\\\"],\\n\",\n",
    "    \")\\n\",\n",
    "    \"ds_test.set_format(\\n\",\n",
    "    \"    type=\\\"torch\\\",\\n\",\n",
    "    \"    columns=[\\\"input_ids\\\", \\\"token_type_ids\\\", \\\"attention_mask\\\", \\\"labels\\\"],\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"ds_train example: {ds_train[0]}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Shuffle training data\\n\",\n",
    "    \"ds_train_shuffle = ds_train.shuffle(seed=42)\\n\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.456\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m10\\u001B[0m - \\u001B[1mAvailable columns: ['cryptopanic_id', 'title', 'description', 'source_domain', 'published_at', 'cryptopanic_url', 'currencies', 'sentiment']\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.457\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m11\\u001B[0m - \\u001B[1mDataset shape: (19468, 8)\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.458\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m17\\u001B[0m - \\u001B[1mUsing text column: 'description' and label column: 'sentiment'\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.460\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m18\\u001B[0m - \\u001B[1mUnique labels: {'Positive': 12461, 'Negative': 4977, 'Neutral': 2030}\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:36.529\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m45\\u001B[0m - \\u001B[1mTrain size: 11680, Val size: 3893, Test size: 3895\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:39.173\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m59\\u001B[0m - \\u001B[1mLabel mappings: {'Negative': 0, 'Neutral': 1, 'Positive': 2}\\u001B[0m\\n\",\n",
    "      \"Map: 100%|██████████| 11680/11680 [00:00<00:00, 158451.21 examples/s]\\n\",\n",
    "      \"Map: 100%|██████████| 3893/3893 [00:00<00:00, 158120.05 examples/s]\\n\",\n",
    "      \"Map: 100%|██████████| 3895/3895 [00:00<00:00, 132375.15 examples/s]\\n\",\n",
    "      \"Map: 100%|██████████| 11680/11680 [00:00<00:00, 15894.67 examples/s]\\n\",\n",
    "      \"Map: 100%|██████████| 3893/3893 [00:00<00:00, 16206.63 examples/s]\\n\",\n",
    "      \"Map: 100%|██████████| 3895/3895 [00:00<00:00, 18636.16 examples/s]\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:39:40.515\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m97\\u001B[0m - \\u001B[1mds_train example: {'labels': tensor(0), 'input_ids': tensor([    3,    59,    74, 30805,  7477,   974,  9469, 28746,   268,    23,\\n\",\n",
    "      \"            6, 23912, 16807,    52,  1749,    18,  8090,   597, 10916,    63,\\n\",\n",
    "      \"            9,   566,   459, 30805,  1104,   994,   585,   195, 18264,     8,\\n\",\n",
    "      \"         6540,   974,  2354,  1099,   739,     7,   118,    10,    11,  1037,\\n\",\n",
    "      \"           48,     4,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\\n\",\n",
    "      \"            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n\",\n",
    "      \"        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n\",\n",
    "      \"        0, 0, 0, 0, 0, 0, 0, 0])}\\u001B[0m\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 5\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6cbce273\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Define evaluation metrics function to compute balanced accuracy and accuracy scores for model predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"0aec3046\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:40.545661Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:40.541659Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"def compute_metrics(eval_pred):\\n\",\n",
    "    \"    predictions, labels = eval_pred\\n\",\n",
    "    \"    predictions = np.argmax(predictions, axis=1)\\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        \\\"balanced_accuracy\\\": balanced_accuracy_score(predictions, labels),\\n\",\n",
    "    \"        \\\"accuracy\\\": accuracy_score(predictions, labels),\\n\",\n",
    "    \"    }\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 6\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"02fcbf0c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Configure training arguments with hyperparameters for fine-tuning FinBERT model on sentiment analysis task\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"48366ad0\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:40.627897Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:40.569552Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"args = TrainingArguments(\\n\",\n",
    "    \"    output_dir=\\\"temp/\\\",\\n\",\n",
    "    \"    eval_strategy=\\\"epoch\\\",\\n\",\n",
    "    \"    save_strategy=\\\"epoch\\\",\\n\",\n",
    "    \"    logging_strategy=\\\"steps\\\",\\n\",\n",
    "    \"    logging_steps=50,\\n\",\n",
    "    \"    learning_rate=2e-6,\\n\",\n",
    "    \"    per_device_train_batch_size=32,\\n\",\n",
    "    \"    per_device_eval_batch_size=32,\\n\",\n",
    "    \"    num_train_epochs=3,\\n\",\n",
    "    \"    weight_decay=0.1,\\n\",\n",
    "    \"    load_best_model_at_end=True,\\n\",\n",
    "    \"    metric_for_best_model=\\\"balanced_accuracy\\\",\\n\",\n",
    "    \")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 7\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"aac34c6e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Load pre-trained FinBERT model and configure it for sequence classification with custom label mappings\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"57f78774\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:39:41.533868Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:40.643148Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"model = AutoModelForSequenceClassification.from_pretrained(\\n\",\n",
    "    \"    model_name,\\n\",\n",
    "    \"    num_labels=len(label_to_id),\\n\",\n",
    "    \"    id2label=id_to_label,\\n\",\n",
    "    \"    label2id=label_to_id,\\n\",\n",
    "    \")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 8\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4281d522\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Initialize trainer with model and datasets, then fine-tune FinBERT and generate predictions on test set\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"5058f8a7\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:47:53.075941Z\",\n",
    "     \"start_time\": \"2025-07-17T11:39:41.549155Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"trainer = Trainer(\\n\",\n",
    "    \"    model=model,\\n\",\n",
    "    \"    args=args,\\n\",\n",
    "    \"    train_dataset=ds_train_shuffle,\\n\",\n",
    "    \"    eval_dataset=ds_val,\\n\",\n",
    "    \"    compute_metrics=compute_metrics,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainer.train()\\n\",\n",
    "    \"\\n\",\n",
    "    \"predictions = trainer.predict(ds_test)\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ],\n",
    "      \"text/html\": [\n",
    "       \"\\n\",\n",
    "       \"    <div>\\n\",\n",
    "       \"      \\n\",\n",
    "       \"      <progress value='1095' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\\n\",\n",
    "       \"      [1095/1095 07:55, Epoch 3/3]\\n\",\n",
    "       \"    </div>\\n\",\n",
    "       \"    <table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \" <tr style=\\\"text-align: left;\\\">\\n\",\n",
    "       \"      <th>Epoch</th>\\n\",\n",
    "       \"      <th>Training Loss</th>\\n\",\n",
    "       \"      <th>Validation Loss</th>\\n\",\n",
    "       \"      <th>Balanced Accuracy</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0.982200</td>\\n\",\n",
    "       \"      <td>0.940989</td>\\n\",\n",
    "       \"      <td>0.317979</td>\\n\",\n",
    "       \"      <td>0.625482</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>0.936200</td>\\n\",\n",
    "       \"      <td>0.911130</td>\\n\",\n",
    "       \"      <td>0.292900</td>\\n\",\n",
    "       \"      <td>0.627023</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>0.912500</td>\\n\",\n",
    "       \"      <td>0.909911</td>\\n\",\n",
    "       \"      <td>0.281448</td>\\n\",\n",
    "       \"      <td>0.629848</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table><p>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ],\n",
    "      \"text/html\": []\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 9\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dd25bb69\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Log model predictions and ground truth labels for analysis and debugging purposes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"d573898b\",\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:47:53.127307Z\",\n",
    "     \"start_time\": \"2025-07-17T11:47:53.121680Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"logger.info(f\\\"Raw logits/predictions from the model: {predictions[0]}\\\")\\n\",\n",
    "    \"logger.info(f\\\"Labels from the dataset: {predictions[1]}\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001B[32m2025-07-17 18:47:53.122\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m1\\u001B[0m - \\u001B[1mRaw logits/predictions from the model: [[-0.55833644 -0.18570296  1.1872989 ]\\n\",\n",
    "      \" [-0.56787324 -0.7137708   2.1606197 ]\\n\",\n",
    "      \" [-0.13270241 -0.444199    1.428188  ]\\n\",\n",
    "      \" ...\\n\",\n",
    "      \" [-0.0583357  -1.1820269   0.31701052]\\n\",\n",
    "      \" [-0.01474534 -0.6527146   0.16227673]\\n\",\n",
    "      \" [-0.31564218 -1.3497086   0.7305849 ]]\\u001B[0m\\n\",\n",
    "      \"\\u001B[32m2025-07-17 18:47:53.124\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m2\\u001B[0m - \\u001B[1mLabels from the dataset: [1 2 2 ... 2 0 2]\\u001B[0m\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 10\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-07-17T11:47:53.581691Z\",\n",
    "     \"start_time\": \"2025-07-17T11:47:53.180784Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"# Create directory for saving the model components\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"output_dir = \\\"../models/finbert_bitcoin_sentiment\\\"\\n\",\n",
    "    \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save the fine-tuned model\\n\",\n",
    "    \"trainer.model.save_pretrained(output_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save the tokenizer\\n\",\n",
    "    \"tokenizer.save_pretrained(output_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save label mappings for later reference\\n\",\n",
    "    \"with open(f\\\"{output_dir}/label_mappings.json\\\", \\\"w\\\") as f:\\n\",\n",
    "    \"    json.dump({\\\"id_to_label\\\": id_to_label, \\\"label_to_id\\\": label_to_id}, f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"Model and tokenizer saved to {output_dir}\\\")\"\n",
    "   ],\n",
    "   \"id\": \"fe455357474ac1e3\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001B[32m2025-07-17 18:47:53.578\\u001B[0m | \\u001B[1mINFO    \\u001B[0m | \\u001B[36m__main__\\u001B[0m:\\u001B[36m<module>\\u001B[0m:\\u001B[36m18\\u001B[0m - \\u001B[1mModel and tokenizer saved to ../models/finbert_bitcoin_sentiment\\u001B[0m\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 11\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \".venv\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "6ec1edb3e04e166"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
